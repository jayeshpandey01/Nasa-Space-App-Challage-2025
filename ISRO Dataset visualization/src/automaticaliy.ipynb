{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66830f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad72e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "📁 No new files to download. Everything is up to date.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configurations ---\n",
    "BASE_URL = \"https://pradan.issdc.gov.in/al1/protected/browse.xhtml?id=suit\"\n",
    "LOCAL_DIR = \"fits_images\"\n",
    "CSV_FILE = \"fits_image_log.csv\"\n",
    "USER_AGENT = \"Mozilla/5.0\"\n",
    "\n",
    "# --- Ensure directory exists ---\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "# --- Initialize existing image set from directory ---\n",
    "existing_files = set(os.listdir(LOCAL_DIR))\n",
    "\n",
    "# --- Load existing CSV if exists ---\n",
    "if os.path.exists(CSV_FILE):\n",
    "    existing_df = pd.read_csv(CSV_FILE)\n",
    "    downloaded_files = set(existing_df['filename'])\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=['filename', 'url', 'date_downloaded'])\n",
    "    downloaded_files = set()\n",
    "\n",
    "# --- Request the page ---\n",
    "headers = {\"User-Agent\": USER_AGENT}\n",
    "response = requests.get(BASE_URL, headers=headers)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# --- Extract .fits links ---\n",
    "new_entries = []\n",
    "for a_tag in soup.find_all(\"a\", href=True):\n",
    "    href = a_tag[\"href\"]\n",
    "    if href.endswith(\".fits\"):\n",
    "        fits_url = urljoin(BASE_URL, href)\n",
    "        filename = href.split(\"/\")[-1]\n",
    "\n",
    "        # Skip if already downloaded\n",
    "        if filename in existing_files or filename in downloaded_files:\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading: {filename}\")\n",
    "        try:\n",
    "            file_data = requests.get(fits_url, stream=True)\n",
    "            file_path = os.path.join(LOCAL_DIR, filename)\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                for chunk in file_data.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "\n",
    "            # Add entry to CSV data\n",
    "            new_entries.append({\n",
    "                \"filename\": filename,\n",
    "                \"url\": fits_url,\n",
    "                \"date_downloaded\": datetime.now().isoformat()\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {filename}: {e}\")\n",
    "\n",
    "# --- Append to or create CSV ---\n",
    "if new_entries:\n",
    "    new_df = pd.DataFrame(new_entries)\n",
    "    final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    final_df.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"\\n✅ CSV updated with {len(new_entries)} new entries.\")\n",
    "else:\n",
    "    print(\"\\n📁 No new files to download. Everything is up to date.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4d30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9adcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"username\"]\"}\n  (Session info: chrome=137.0.7151.119); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x0x7ff7d8ddcda5+78885]\n\tGetHandleVerifier [0x0x7ff7d8ddce00+78976]\n\t(No symbol) [0x0x7ff7d8b99bca]\n\t(No symbol) [0x0x7ff7d8bf0766]\n\t(No symbol) [0x0x7ff7d8bf0a1c]\n\t(No symbol) [0x0x7ff7d8c44467]\n\t(No symbol) [0x0x7ff7d8c18bcf]\n\t(No symbol) [0x0x7ff7d8c4122f]\n\t(No symbol) [0x0x7ff7d8c18963]\n\t(No symbol) [0x0x7ff7d8be16b1]\n\t(No symbol) [0x0x7ff7d8be2443]\n\tGetHandleVerifier [0x0x7ff7d90b4eed+3061101]\n\tGetHandleVerifier [0x0x7ff7d90af33d+3037629]\n\tGetHandleVerifier [0x0x7ff7d90ce592+3165202]\n\tGetHandleVerifier [0x0x7ff7d8df730e+186766]\n\tGetHandleVerifier [0x0x7ff7d8dfeb3f+217535]\n\tGetHandleVerifier [0x0x7ff7d8de59b4+114740]\n\tGetHandleVerifier [0x0x7ff7d8de5b69+115177]\n\tGetHandleVerifier [0x0x7ff7d8dcc368+10728]\n\tBaseThreadInitThunk [0x0x7ffa8809e8d7+23]\n\tRtlUserThreadStart [0x0x7ffa8a0bc34c+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoSuchElementException\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m driver.get(\u001b[33m\"\u001b[39m\u001b[33mhttps://pradan.issdc.gov.in/login.xhtml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m time.sleep(\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43musername\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.send_keys(\u001b[33m\"\u001b[39m\u001b[33mjayeshpandey754@gmail.com\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m driver.find_element(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m).send_keys(\u001b[33m\"\u001b[39m\u001b[33mJayesh@9930\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m driver.find_element(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mloginButton\u001b[39m\u001b[33m\"\u001b[39m).click()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:914\u001b[39m, in \u001b[36mWebDriver.find_element\u001b[39m\u001b[34m(self, by, value)\u001b[39m\n\u001b[32m    911\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby.root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43musing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:447\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    445\u001b[39m response = \u001b[38;5;28mself\u001b[39m.command_executor.execute(driver_command, params)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mNoSuchElementException\u001b[39m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"username\"]\"}\n  (Session info: chrome=137.0.7151.119); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x0x7ff7d8ddcda5+78885]\n\tGetHandleVerifier [0x0x7ff7d8ddce00+78976]\n\t(No symbol) [0x0x7ff7d8b99bca]\n\t(No symbol) [0x0x7ff7d8bf0766]\n\t(No symbol) [0x0x7ff7d8bf0a1c]\n\t(No symbol) [0x0x7ff7d8c44467]\n\t(No symbol) [0x0x7ff7d8c18bcf]\n\t(No symbol) [0x0x7ff7d8c4122f]\n\t(No symbol) [0x0x7ff7d8c18963]\n\t(No symbol) [0x0x7ff7d8be16b1]\n\t(No symbol) [0x0x7ff7d8be2443]\n\tGetHandleVerifier [0x0x7ff7d90b4eed+3061101]\n\tGetHandleVerifier [0x0x7ff7d90af33d+3037629]\n\tGetHandleVerifier [0x0x7ff7d90ce592+3165202]\n\tGetHandleVerifier [0x0x7ff7d8df730e+186766]\n\tGetHandleVerifier [0x0x7ff7d8dfeb3f+217535]\n\tGetHandleVerifier [0x0x7ff7d8de59b4+114740]\n\tGetHandleVerifier [0x0x7ff7d8de5b69+115177]\n\tGetHandleVerifier [0x0x7ff7d8dcc368+10728]\n\tBaseThreadInitThunk [0x0x7ffa8809e8d7+23]\n\tRtlUserThreadStart [0x0x7ffa8a0bc34c+44]\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium --quiet\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time, os, requests, pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Setup\n",
    "BASE_URL = \"https://pradan.issdc.gov.in/al1/protected/browse.xhtml?id=suit\"\n",
    "LOCAL_DIR = \"fits_images\"\n",
    "CSV_FILE = \"fits_image_log.csv\"\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "# Load existing files and CSV\n",
    "existing = set(os.listdir(LOCAL_DIR))\n",
    "df = pd.read_csv(CSV_FILE) if os.path.exists(CSV_FILE) else pd.DataFrame(columns=[\"filename\", \"url\", \"date_downloaded\"])\n",
    "downloaded = set(df.filename)\n",
    "\n",
    "# Start headless browser\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# 1. Login workflow (customize this to match the actual login form)\n",
    "driver.get(\"https://pradan.issdc.gov.in/login.xhtml\")\n",
    "time.sleep(2)\n",
    "driver.find_element(\"id\", \"username\").send_keys(\"jayeshpandey754@gmail.com\")\n",
    "driver.find_element(\"id\", \"password\").send_keys(\"Jayesh@9930\")\n",
    "driver.find_element(\"id\", \"loginButton\").click()\n",
    "time.sleep(5)  # wait for login to complete\n",
    "\n",
    "# 2. Browse target\n",
    "driver.get(BASE_URL)\n",
    "time.sleep(5)  # wait for JS to load\n",
    "\n",
    "# 3. Extract .fits links\n",
    "links = driver.find_elements(\"xpath\", \"//a[contains(@href, '.fits')]\")\n",
    "new_entries = []\n",
    "for a in links:\n",
    "    href = a.get_attribute(\"href\")\n",
    "    filename = href.split(\"/\")[-1]\n",
    "    if filename in existing or filename in downloaded:\n",
    "        continue\n",
    "    print(\"Downloading:\", filename)\n",
    "    resp = requests.get(href, stream=True)\n",
    "    path = os.path.join(LOCAL_DIR, filename)\n",
    "    with open(path, \"wb\") as f:\n",
    "        for chunk in resp.iter_content(8192):\n",
    "            f.write(chunk)\n",
    "    new_entries.append({\"filename\": filename, \"url\": href, \"date_downloaded\": datetime.now().isoformat()})\n",
    "\n",
    "# 4. Update CSV\n",
    "if new_entries:\n",
    "    df = pd.concat([df, pd.DataFrame(new_entries)], ignore_index=True)\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "    print(\"✅ Downloaded\", len(new_entries), \"new files.\")\n",
    "else:\n",
    "    print(\"📁 No new files found.\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bdff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Fetching page: https://pradan.issdc.gov.in/ch2/protected/payload.xhtml\n",
      "\n",
      "📁 No new FITS files found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- Configurations ---\n",
    "SH_FILE = \"suit_2025Jun23T134658908.sh\"  # Replace with your actual .sh file path\n",
    "DOWNLOAD_DIR = \"fits_images\"\n",
    "CSV_LOG = \"fits_image_log.csv\"\n",
    "\n",
    "# --- Step 1: Read & parse the .sh file ---\n",
    "with open(SH_FILE, \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Extract cookies\n",
    "cookie_match = re.search(r'cookies\\s*=\\s*\"(.*?)\"', content, re.DOTALL)\n",
    "cookie_raw = cookie_match.group(1).strip().replace(\"\\n\", \"\") if cookie_match else \"\"\n",
    "cookies = dict(item.strip().split(\"=\", 1) for item in cookie_raw.strip(\";\").split(\";\") if \"=\" in item)\n",
    "\n",
    "# Extract base URL\n",
    "url_prefix = re.search(r'urlPrefix\\s*=\\s*\"(.*?)\"', content).group(1)\n",
    "\n",
    "# Extract page URL if present\n",
    "page_match = re.search(r'\\$urlPrefix\"([^\"]+)\"', content)\n",
    "page_path = page_match.group(1) if page_match else \"/al1/protected/browse.xhtml?id=suit\"\n",
    "\n",
    "full_url = urljoin(url_prefix, page_path)\n",
    "\n",
    "# --- Step 2: Setup ---\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "existing_files = set(os.listdir(DOWNLOAD_DIR))\n",
    "\n",
    "if os.path.exists(CSV_LOG):\n",
    "    df_log = pd.read_csv(CSV_LOG)\n",
    "    downloaded = set(df_log['filename'])\n",
    "else:\n",
    "    df_log = pd.DataFrame(columns=[\"filename\", \"url\", \"date_downloaded\"])\n",
    "    downloaded = set()\n",
    "\n",
    "# --- Step 3: Fetch the HTML page ---\n",
    "print(f\"🔗 Fetching page: {full_url}\")\n",
    "response = requests.get(full_url, cookies=cookies, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"❌ Failed to fetch page (Status {response.status_code})\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# --- Step 4: Find and download .fits files ---\n",
    "new_entries = []\n",
    "\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a['href']\n",
    "    if href.endswith(\".fits\"):\n",
    "        fits_url = urljoin(url_prefix, href)\n",
    "        filename = os.path.basename(href)\n",
    "\n",
    "        if filename in existing_files or filename in downloaded:\n",
    "            print(f\"⏩ Skipping (exists): {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"⬇️  Downloading: {filename}\")\n",
    "        try:\n",
    "            r = requests.get(fits_url, stream=True, cookies=cookies)\n",
    "            path = os.path.join(DOWNLOAD_DIR, filename)\n",
    "            with open(path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "\n",
    "            new_entries.append({\n",
    "                \"filename\": filename,\n",
    "                \"url\": fits_url,\n",
    "                \"date_downloaded\": datetime.now().isoformat()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to download {filename}: {e}\")\n",
    "\n",
    "# --- Step 5: Update CSV log ---\n",
    "if new_entries:\n",
    "    df_new = pd.DataFrame(new_entries)\n",
    "    df_log = pd.concat([df_log, df_new], ignore_index=True)\n",
    "    df_log.to_csv(CSV_LOG, index=False)\n",
    "    print(f\"\\n✅ CSV log updated with {len(new_entries)} new files.\")\n",
    "else:\n",
    "    print(\"\\n📁 No new FITS files found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd870f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
